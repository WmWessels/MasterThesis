{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.openml.org/api/v1/xml\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "import time\n",
    "print(openml.config.server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "automlbench_dids = [41021, 42225, 42571, 4549, 42572, 42570, 42705,\n",
    "                   42728, 550, 546, 541, 507, 505, 287, 216, 41540,\n",
    "                   42688, 422, 416, 42724, 42727, 42729, 42726, 42730,\n",
    "                   201, 41980, 42731, 531, 42563, 574, 3050, 3277, 43071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = openml.datasets.list_datasets()\n",
    "to_consider = openml.datasets.list_datasets()\n",
    "TOTAL = len(all_datasets)\n",
    "# print(TOTAL)\n",
    "# # all_tasks = openml.tasks.list_tasks()\n",
    "to_remove = set()\n",
    "# # print(len(all_tasks))\n",
    "# print(\"From:\", len(to_consider)) # 3004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 5210\n",
      "To: 5177\n"
     ]
    }
   ],
   "source": [
    "# remove datasets from the datasets list which are in the automl bench or close relatives\n",
    "print(\"From:\", len(to_consider))\n",
    "\n",
    "# remove exact duplicates\n",
    "for did in all_datasets:\n",
    "    if did in automlbench_dids:\n",
    "        to_remove.add(did)\n",
    "\n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "assert len(to_consider) == len(all_datasets) - len(automlbench_dids)\n",
    "\n",
    "print(\"To:\", len(to_consider)) # 2965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 5177\n",
      "To: 1510\n"
     ]
    }
   ],
   "source": [
    "# Remove datasets with too few features, instances or classes\n",
    "print(\"From:\", len(to_consider))\n",
    "\n",
    "for did in all_datasets:\n",
    "    try:\n",
    "        if all_datasets[did]['NumberOfInstances'] * all_datasets[did]['NumberOfFeatures'] < 2_000:\n",
    "            to_remove.add(did)\n",
    "        elif all_datasets[did]['NumberOfInstances'] >= 500_000:\n",
    "            to_remove.add(did)\n",
    "        elif all_datasets[did]['NumberOfInstances'] < 20:\n",
    "            to_remove.add(did)\n",
    "        elif all_datasets[did]['NumberOfFeatures'] < 2:\n",
    "            to_remove.add(did)\n",
    "        elif all_datasets[did]['NumberOfFeatures'] > 2500:\n",
    "            to_remove.add(did)\n",
    "        elif all_datasets[did]['NumberOfClasses'] > 1:\n",
    "            to_remove.add(did)\n",
    "        elif all_datasets[did]['NumberOfInstances'] * all_datasets[did]['NumberOfFeatures'] > 10_000_000:\n",
    "            to_remove.add(did)\n",
    "    except KeyError:\n",
    "        #print(did, all_datasets[did])\n",
    "        to_remove.add(did)\n",
    "        \n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "        \n",
    "print(\"To:\", len(to_consider)) # 651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets_as_frame = pd.DataFrame.from_dict(all_datasets, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 1510\n",
      "To: 1503\n"
     ]
    }
   ],
   "source": [
    "# Remove all datasets with \"BNG\" in name\n",
    "print(\"From:\", len(to_consider))\n",
    "\n",
    "for did in to_consider:\n",
    "    if to_consider[did]['name'].startswith(\"BNG\"):\n",
    "        to_remove.add(did)\n",
    "        \n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "        \n",
    "print(\"To:\", len(to_consider)) # 641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for similar names and manually check overlaps\n",
    "# def return_triplets(name):\n",
    "#     triplets = set()\n",
    "#     name = name.lower()\n",
    "#     for i in range(len(name) - 2):\n",
    "#         triplets.add(name[i: i+3])\n",
    "#     return triplets\n",
    "\n",
    "# manually_look_at = []\n",
    "# for did in to_consider:\n",
    "#     for did2 in automlbench_dids:\n",
    "#         automl_triplets = return_triplets(all_datasets[did2]['name'])\n",
    "#         other_triplets = return_triplets(all_datasets[did]['name'])\n",
    "#         n_hits = sum([trip in other_triplets for trip in automl_triplets])\n",
    "#         ratio = n_hits / len(automl_triplets)\n",
    "#         if n_hits > 2:\n",
    "#             manually_look_at.append([n_hits, ratio, all_datasets[did]['name'], all_datasets[did2]['name'], did, did2])\n",
    "\n",
    "# df = pd.DataFrame(manually_look_at)\n",
    "# with open(\"Manualautomlregr.csv\", \"w\") as fh:\n",
    "#    fh.write(df.to_csv())     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 1503\n",
      "To: 1438\n"
     ]
    }
   ],
   "source": [
    "# This list contains all manually collected dataset ids as in \n",
    "# https://docs.google.com/spreadsheets/d/1XH1mQFq7-1fs28F-CQJo6i6Wg7IlpDS0aDTAJBoaD7k/edit#gid=0\n",
    "print(\"From:\", len(to_consider))\n",
    "for did in [209, 543, 4545, 42092, 42635, 42723, 43465, 43984, 43986, \n",
    "            43989, 43992, 43994, 43996, 43997, 43998, 43999, 44002, \n",
    "            44009, 44011, 44013, 44014, 44015, 44016, 44019, 44039, \n",
    "            44042, 44044, 44047, 44049, 44051, 44054, 44057, 44059, \n",
    "            44061, 44062, 44063, 44064, 44066, 44134, 44136, 44138, \n",
    "            44139, 44140, 4414, 44142, 44144, 44152, 44201, 44202, \n",
    "            44203, 44269, 44956, 44979, 44985, 44987, 44988, 44990, \n",
    "            45031, 45032, 45033, 45041, 45042, 42713, 44000, 44017, 44048, 44063, 44142]:\n",
    "    to_remove.add(did)\n",
    " \n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "print(\"To:\", len(to_consider)) # 603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for similar metafeatures and manually check overlaps\n",
    "# metafeature_names = [\n",
    "#     'MaxNominalAttDistinctValues', 'NumberOfClasses',\n",
    "#     'NumberOfFeatures', 'NumberOfInstances', 'NumberOfInstancesWithMissingValues', 'NumberOfMissingValues',\n",
    "#     'NumberOfNumericFeatures', 'NumberOfSymbolicFeatures',\n",
    "# ]\n",
    "\n",
    "# simple_metafeatures_to_consider = pd.DataFrame(\n",
    "#     {k: v for k, v in all_datasets.items() if k in to_consider}\n",
    "# ).transpose()[metafeature_names]\n",
    "# simple_metafeatures_automl = pd.DataFrame(\n",
    "#     {k: v for k, v in all_datasets.items() if k in automlbench_dids}\n",
    "# ).transpose()[metafeature_names]\n",
    "\n",
    "# # Checking the hamming distance of the datasets\n",
    "# manually_look_at = []\n",
    "# for did1, mf1 in simple_metafeatures_to_consider.iterrows():\n",
    "#     for did2, mf2 in simple_metafeatures_automl.iterrows():\n",
    "#         n_hits = np.sum(mf1 == mf2)\n",
    "#         ratio = n_hits / len(mf1)\n",
    "#         n_hits = np.sum(\n",
    "#             mf1[['NumberOfFeatures', 'NumberOfInstances']] == mf2[['NumberOfFeatures', 'NumberOfInstances']]\n",
    "#         )\n",
    "#         if ratio > 0.5 or n_hits == 2:\n",
    "#             entry = [ratio, n_hits, all_datasets[did1]['name'], all_datasets[did2]['name'], did1, did2]\n",
    "#             mfs = np.array([[mf1[n], mf2[n]] for n in metafeature_names]).flatten()\n",
    "#             entry.extend(mfs)\n",
    "#             manually_look_at.append(entry)\n",
    "\n",
    "# header = [\"ratio\", \"n_hits\", \"name\", \"automl name\", \"did\", \"automl did\"] \n",
    "# mfs = list(np.array([[m, m] for m in metafeature_names]).flatten())\n",
    "# df = pd.DataFrame(manually_look_at, columns = header + mfs)\n",
    "# # with open(\"ManualMetafeaturesregr.csv\", \"w\") as fh:\n",
    "# #    fh.write(df.to_csv())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 1438\n",
      "To: 1419\n"
     ]
    }
   ],
   "source": [
    "# This list contains all manually collected dataset ids as in \n",
    "# https://docs.google.com/spreadsheets/d/1SWEBsFTykdUMq-wBcx9NHchhdolB632EJYaAmQwwQtM/edit#gid=0\n",
    "print(\"From:\", len(to_consider))\n",
    "for did in [315, 580, 610, 652, 41938, 41968, 41969, 43070, 43672, 43888, 43983, 43991,\n",
    "            44008, 44133, 44141, 44971, 44972, 44989, 45030]:\n",
    "    to_remove.add(did)\n",
    " \n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "print(\"To:\", len(to_consider)) # 599\n",
    "# check whether we can work usefully with creditcard (1597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>NumberOfFeatures</th>\n",
       "      <th>NumberOfInstances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>liver-disorders</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>kin8nm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>cleveland</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>auto_price</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45076</th>\n",
       "      <td>churn</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45078</th>\n",
       "      <td>nomao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>34465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45079</th>\n",
       "      <td>road-safety</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>111762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45081</th>\n",
       "      <td>Tallo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>307014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45108</th>\n",
       "      <td>DutchTwitterDataset</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>451200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1419 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name NumberOfClasses NumberOfFeatures NumberOfInstances\n",
       "8          liver-disorders             0.0              6.0             345.0\n",
       "189                 kin8nm             0.0              9.0            8192.0\n",
       "191              wisconsin             0.0             33.0             194.0\n",
       "194              cleveland             0.0             14.0             303.0\n",
       "195             auto_price             0.0             16.0             159.0\n",
       "...                    ...             ...              ...               ...\n",
       "45076                churn             0.0             21.0            5000.0\n",
       "45078                nomao             0.0            119.0           34465.0\n",
       "45079          road-safety             0.0             33.0          111762.0\n",
       "45081                Tallo             0.0             21.0          307014.0\n",
       "45108  DutchTwitterDataset             0.0             20.0          451200.0\n",
       "\n",
       "[1419 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 25)\n",
    "columns = ['name', 'NumberOfClasses', 'NumberOfFeatures', 'NumberOfInstances', ]\n",
    "\n",
    "to_consider_stats = pd.DataFrame(\n",
    "    {k: v for k, v in all_datasets.items() if k in to_consider}\n",
    ").transpose()[columns]\n",
    "\n",
    "to_consider_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 1419\n",
      "0.03sec: 25/1419\n",
      "0.08sec: 50/1419\n",
      "0.11sec: 75/1419\n",
      "0.14sec: 100/1419\n",
      "0.16sec: 125/1419\n",
      "0.19sec: 150/1419\n",
      "2.97sec: 175/1419\n",
      "3.03sec: 200/1419\n",
      "3.19sec: 225/1419\n",
      "3.27sec: 250/1419\n",
      "3.34sec: 275/1419\n",
      "3.42sec: 300/1419\n",
      "3.49sec: 325/1419\n",
      "3.57sec: 350/1419\n",
      "3.74sec: 375/1419\n",
      "3.82sec: 400/1419\n",
      "3.89sec: 425/1419\n",
      "3.97sec: 450/1419\n",
      "4.13sec: 475/1419\n",
      "4.21sec: 500/1419\n",
      "4.29sec: 525/1419\n",
      "4.36sec: 550/1419\n",
      "4.44sec: 575/1419\n",
      "4.59sec: 600/1419\n",
      "4.67sec: 625/1419\n",
      "4.74sec: 650/1419\n",
      "4.89sec: 675/1419\n",
      "4.97sec: 700/1419\n",
      "5.05sec: 725/1419\n",
      "5.12sec: 750/1419\n",
      "5.19sec: 775/1419\n",
      "5.26sec: 800/1419\n",
      "5.42sec: 825/1419\n",
      "5.49sec: 850/1419\n",
      "5.57sec: 875/1419\n",
      "5.65sec: 900/1419\n",
      "5.73sec: 925/1419\n",
      "5.91sec: 950/1419\n",
      "5.98sec: 975/1419\n",
      "6.06sec: 1000/1419\n",
      "6.14sec: 1025/1419\n",
      "6.29sec: 1050/1419\n",
      "6.36sec: 1075/1419\n",
      "6.43sec: 1100/1419\n",
      "6.51sec: 1125/1419\n",
      "6.66sec: 1150/1419\n",
      "36.39sec: 1175/1419\n",
      "44.41sec: 1200/1419\n",
      "51.89sec: 1225/1419\n",
      "75.11sec: 1250/1419\n",
      "92.10sec: 1275/1419\n",
      "104.46sec: 1300/1419\n",
      "126.36sec: 1325/1419\n",
      "139.52sec: 1350/1419\n",
      "146.71sec: 1375/1419\n",
      "157.46sec: 1400/1419\n",
      "To: 1344\n"
     ]
    }
   ],
   "source": [
    "# # remove sparse datasets\n",
    "print(\"From:\", len(to_consider))\n",
    "start = time.time()\n",
    "for i, did in enumerate(to_consider):\n",
    "    #print(did)\n",
    "    if (i+1) % 25 == 0:\n",
    "        print(\"%4.2fsec: %d/%d\" % (time.time() - start, i+1, len(to_consider)))\n",
    "    d = openml.datasets.get_dataset(int(did), download_data=False)\n",
    "    #print(\n",
    "    #    d.format.lower(), \n",
    "    #    'sparse' in d.format.lower(), \n",
    "    #    d.description and 'CLASSINDEX: none specific' in d.description\n",
    "    #)\n",
    "    # if 'sparse' in d.format.lower():\n",
    "    #     to_remove.add(did)\n",
    "    # if d.description and 'CLASSINDEX: none specific' in d.description:\n",
    "    #     to_remove.add(did)\n",
    "    for f in d.features.values():\n",
    "        if f.data_type == 'string':\n",
    "            to_remove.add(did)\n",
    "\n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "print(\"To:\", len(to_consider)) # 599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove identical/similar datasets (to reduce the load computing the meta-data)\n",
    "# # check for similar names and manually check overlaps\n",
    "# def return_triplets(name):\n",
    "#     triplets = set()\n",
    "#     name = name.lower()\n",
    "#     for i in range(len(name) - 2):\n",
    "#         triplets.add(name[i: i+3])\n",
    "#     return triplets\n",
    "\n",
    "# #Remove QSAR datasets\n",
    "\n",
    "# manually_look_at = []\n",
    "# for i, did in enumerate(to_consider):\n",
    "#     for j, did2 in enumerate(to_consider):\n",
    "        \n",
    "#         if all_datasets[did][\"name\"].startswith(\"QSAR\") or all_datasets[did2][\"name\"].startswith(\"QSAR\"):\n",
    "#             continue\n",
    "#         if j <= i:\n",
    "#             continue\n",
    "#         if len(all_datasets[did2]['name']) < 3:\n",
    "#             continue\n",
    "#         other2_triplets = return_triplets(all_datasets[did2]['name'])\n",
    "#         other_triplets = return_triplets(all_datasets[did]['name'])\n",
    "#         n_hits = sum([trip in other_triplets for trip in other2_triplets])\n",
    "#         ratio = n_hits / len(other2_triplets)\n",
    "#         if n_hits > 2:\n",
    "#             manually_look_at.append([n_hits, ratio, all_datasets[did]['name'], all_datasets[did2]['name'], did, did2])\n",
    "\n",
    "# df = pd.DataFrame(manually_look_at)\n",
    "# with open(\"manual_check.csv\", \"w\") as fh:\n",
    "#    fh.write(df.to_csv())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 1344\n",
      "To: 1238\n"
     ]
    }
   ],
   "source": [
    "print(\"From:\", len(to_consider))\n",
    "for did in [45013, 45014, 44980, 195, 42372, 573, 43981, 43982, 43990,\n",
    "            44007, 44132, 45029, 44978, 43882, 42900, 562, 43987, 43995, 44012, 44137,\n",
    "            555, 556, 44055, 43127, 504, 568, 570, 575, 576, 577, 578, 670, 671, 672,\n",
    "            44041, 44056, 43056, 1436, 1435, 44965, 44003, 44020, 23515,42436, 42437, \n",
    "            42438, 42439, 42444, 42445, 41514, 41515, 41516, 41517, 41518, 41519, 43384, \n",
    "            43483, 43582, 43943, 43959, 44187, 44191, 44192, 44194, 44223, 41516, 41517, 43123, \n",
    "            41523, 41524, 41525, 43127, 44236, 41928, 42131, 42559, 42720, 43928, 44004, 44021, \n",
    "            44146, 45048, 42559, 42720, 43881, 43882, 44235, 43127, 44068, 44069, 43144, 44069, 43871, 44967,\n",
    "            42352, 44029, 44069, 44962, 43440, 42437, 42438, 42439, 42444, 42445, 45043, 42822, 43682, 43050,\n",
    "            43056, 44957, 43079, 44055, 44005, 44022, 44147, 44983, 44006, 44023, 44148, 44964, 43180, 43190, \n",
    "            43192, 43194, 43198, 43256, 44179, 44180, 44214, 44218, 44219, 44220, 44221, 44267, 43483, 43582,\n",
    "            43874, 43403, 43665, 43759, 43466, 43692, 44976, 43962, 44960, 44025, 43939, 43979, 44024, 44031, \n",
    "            44090, 44362, 44363, 44364, 44365, 44366, 44977, 45025, 44053, 44252, 44280, 44243, 44252, 44280, \n",
    "            44794, 44296, 44310, 45015] :\n",
    "    to_remove.add(did)\n",
    " \n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "print(\"To:\", len(to_consider)) # 503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[581, 582, 583, 584, 586, 587, 588, 589, 590, 592, 593, 595, 597, 598, 599, 600, 602, 603, 604, 605, 606, 607, 608, 609, 612, 614, 615, 616, 617, 618, 619, 620, 622, 623, 625, 626, 627, 628, 629, 630, 631, 632, 633, 635, 636, 637, 638, 639, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 653, 654, 655, 657, 658]\n",
      "[3040, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3051, 3052, 3053, 3054, 3056, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3084, 3085, 3086, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3096, 3097, 3098, 3100, 3101, 3103, 3104, 3105, 3106, 3107, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3169, 3171, 3172, 3173, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3187, 3189, 3190, 3191, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3224, 3225, 3226, 3227, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3239, 3240, 3241, 3242, 3243, 3245, 3246, 3247, 3248, 3249, 3251, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3311, 3312, 3315, 3316, 3317, 3318, 3319, 3321, 3322, 3323, 3324, 3326, 3327, 3328, 3329, 3330, 3331, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3341, 3342, 3343, 3344, 3345, 3346, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3370, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3436, 3437, 3438, 3439, 3440, 3441, 3443, 3444, 3445, 3447, 3448, 3449, 3450, 3452, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3495, 3496, 3497, 3498, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3510, 3511, 3512, 3513, 3515, 3516, 3517, 3518, 3519, 3520, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3538, 3539, 3540, 3541, 3542, 3544, 3545, 3546, 3547, 3548, 3551, 3552, 3553, 3554, 3555, 3556, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3580, 3581, 3583, 3584, 3585, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3621, 3622, 3623, 3624, 3626, 3628, 3629, 3630, 3631, 3632, 3633, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3644, 3645, 3646, 3647, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3671, 3672, 3673, 3674, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3724, 3725, 3726, 3727, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3737, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3771, 3772, 3773, 3775, 3776, 3777, 3778, 3779, 3780, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3807, 3809, 3810, 3811, 3812, 3813, 3814, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3845, 3847, 3848, 3849, 3850, 3851, 3852, 3854, 3855, 3856, 3857, 3858, 3859, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3904, 3905, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3947, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3999, 4000, 4002, 4003, 4004, 4005, 4006, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4071, 4072, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4096, 4097, 4098, 4100, 4101, 4102, 4103, 4104, 4105, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4131, 44028, 44970]\n",
      "To: 258\n"
     ]
    }
   ],
   "source": [
    "# Subsample from 'topics', where there are dozens of super-similar\n",
    "# datasets from one source\n",
    "dataset_names = {did: value['name'] for did, value in all_datasets.items()\n",
    "                 if did in to_consider}\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "friedman_datasets = [\n",
    "    did for did, name in dataset_names.items() if name.startswith('fri_c')\n",
    "]\n",
    "print(friedman_datasets)\n",
    "\n",
    "qsar_datasets =  [\n",
    "    did for did, name in dataset_names.items() \n",
    "    if name.startswith('QSAR')\n",
    "]\n",
    "print(qsar_datasets)\n",
    "\n",
    "# print(qsar_datasets)\n",
    "for dataset_ids_to_sample_from, num_keep in (\n",
    "    (friedman_datasets, 10),\n",
    "    # (ova_datasets, 7),\n",
    "    # (volcanoes_datasets, 10),\n",
    "    # (auto_univ_datasets, 4),\n",
    "    (qsar_datasets, 50)\n",
    "):\n",
    "    choices_to_drop = np.random.choice(\n",
    "        dataset_ids_to_sample_from,\n",
    "        replace=False,\n",
    "        size=len(dataset_ids_to_sample_from) - num_keep)\n",
    "    for choice in choices_to_drop:\n",
    "        if choice in to_consider:\n",
    "            del to_consider[choice]\n",
    "print(\"To:\", len(to_consider)) # 454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for similar metafeatures and manually check overlaps\n",
    "# metafeature_names = [\n",
    "#     'MajorityClassSize', 'MaxNominalAttDistinctValues', 'MinorityClassSize', 'NumberOfClasses',\n",
    "#     'NumberOfFeatures', 'NumberOfInstances', 'NumberOfInstancesWithMissingValues', 'NumberOfMissingValues',\n",
    "#     'NumberOfNumericFeatures', 'NumberOfSymbolicFeatures',\n",
    "# ]\n",
    "\n",
    "# simple_metafeatures_to_consider = pd.DataFrame(\n",
    "#     {k: v for k, v in all_datasets.items() if k in to_consider}\n",
    "# ).transpose()[metafeature_names]\n",
    "\n",
    "# # Checking the hamming distance of the datasets\n",
    "# manually_look_at = []\n",
    "# for did1, mf1 in simple_metafeatures_to_consider.iterrows():\n",
    "#     for did2, mf2 in simple_metafeatures_to_consider.iterrows():\n",
    "#         if did2 <= did1:\n",
    "#             continue\n",
    "#         n_hits = np.sum(mf1 == mf2)\n",
    "#         ratio = n_hits / len(mf1)\n",
    "#         n_hits = np.sum(\n",
    "#             mf1[['NumberOfFeatures', 'NumberOfInstances']] == mf2[['NumberOfFeatures', 'NumberOfInstances']]\n",
    "#         )\n",
    "#         if ratio > 0.5 or n_hits == 2:\n",
    "#             entry = [ratio, n_hits, all_datasets[did1]['name'], all_datasets[did2]['name'], did1, did2]\n",
    "#             mfs = np.array([[mf1[n], mf2[n]] for n in metafeature_names]).flatten()\n",
    "#             entry.extend(mfs)\n",
    "#             manually_look_at.append(entry)\n",
    "\n",
    "# header = [\"ratio\", \"n_hits\", \"name\", \"automl name\", \"did\", \"automl did\"] \n",
    "# mfs = list(np.array([[m, m] for m in metafeature_names]).flatten())\n",
    "# df = pd.DataFrame(manually_look_at, columns = header + mfs)\n",
    "# with open(\"ManualMetafeaturesregression.csv\", \"w\") as fh:\n",
    "#    fh.write(df.to_csv()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 258\n",
      "To: 242\n"
     ]
    }
   ],
   "source": [
    "print(\"From:\", len(to_consider))\n",
    "for did in [44026, 45055, 45052, 45053, 45054, 45055, 1027, 43988, 42111, 42112, 42113, 42361,\n",
    "            1424, 1425, 1426, 1427, 1428, 1429,]:\n",
    "    to_remove.add(did)\n",
    " \n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "print(\"To:\", len(to_consider)) # 222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 242\n",
      "To: 214\n"
     ]
    }
   ],
   "source": [
    "print(\"From:\", len(to_consider))\n",
    "\n",
    "for did in [\n",
    "     1414, #bad attribute error when retrieving dataset\n",
    "     43026, #single entry dataset\n",
    "     43034, #single entry dataset\n",
    "     1414, #bucket not accessible\n",
    "     41275, #bucket not accessible\n",
    "     45108, #bucket not accessible\n",
    "     41275, #bucket not accessible\n",
    "     44537, #more classes than instances\n",
    "     42130, #errors in gama\n",
    "     5, #more classes than instances\n",
    "     41700, #timeseries\n",
    "     42803, #too big\n",
    "     664, # copy of other data set\n",
    "     231, #below are all classification tasks with feature NumberofClasses 0\n",
    "     298,\n",
    "     301,\n",
    "     516,\n",
    "     524,\n",
    "     703,\n",
    "     1028,\n",
    "     1097,\n",
    "     1228,\n",
    "     1430,\n",
    "     1432,\n",
    "     1433,\n",
    "     1571,\n",
    "     1572,\n",
    "     1574,\n",
    "     1575,\n",
    "     1579,\n",
    "     1589,\n",
    "     1591,\n",
    "     1593,\n",
    "     4532,\n",
    "     23395,\n",
    "     41463,\n",
    "     41943,\n",
    "     42175,\n",
    "     42176,\n",
    "     42464,\n",
    "     42636,\n",
    "     42889,\n",
    "     42931,\n",
    "]:\n",
    "    to_remove.add(did)\n",
    "    \n",
    "for did in to_remove:\n",
    "    if did in to_consider:\n",
    "        del to_consider[did]\n",
    "print(\"To:\", len(to_consider)) # 209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(\n",
    "    {k: v for k, v in all_datasets.items() if k in to_consider}\n",
    ").transpose()\n",
    "final.drop_duplicates(subset = [\"name\"], keep = \"last\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.to_csv(\"dataset_ids/regression_dids.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('masterthesisenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa1c239501b12b72b3754f84661a3db9994fa2a1e538f4c0273138e3d7831ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
