{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline=Pipeline(steps=[('1', Normalizer(norm='l1')),\n",
      "                ('0',\n",
      "                 ExtraTreesClassifier(bootstrap=True,\n",
      "                                      max_features=0.9500000000000001,\n",
      "                                      min_samples_leaf=10,\n",
      "                                      min_samples_split=13))])\n",
      "Ignoring: list index out of range\n",
      "pipeline=Pipeline(steps=[('imputation', SimpleImputer(strategy='median')),\n",
      "                ('1', Normalizer(norm='l1')),\n",
      "                ('0',\n",
      "                 ExtraTreesClassifier(bootstrap=True,\n",
      "                                      max_features=0.9500000000000001,\n",
      "                                      min_samples_leaf=10,\n",
      "                                      min_samples_split=13))])\n"
     ]
    }
   ],
   "source": [
    "from gama import GamaClassifier\n",
    "from gama.genetic_programming.components import Individual\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clf = GamaClassifier()\n",
    "    individual = Individual.from_string(\n",
    "        \"ExtraTreesClassifier(Normalizer(data, Normalizer.norm='l1'), ExtraTreesClassifier.bootstrap=True, ExtraTreesClassifier.criterion='gini', ExtraTreesClassifier.max_features=0.9500000000000001, min_samples_leaf=10, min_samples_split=13, ExtraTreesClassifier.n_estimators=100))\",\n",
    "        clf._pset,\n",
    "        clf._operator_set._compile,\n",
    "    )\n",
    "    pipeline = individual.pipeline\n",
    "    print(f\"{pipeline=}\")\n",
    "\n",
    "    # it is possible your dataset requires some encoding etc.\n",
    "    # in that case we actually have to call `fit` first for the given dataset,\n",
    "    # but we don't need a successful termination.\n",
    "    # So we can just have it set with a low timeout:\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    x, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "    clf = GamaClassifier(max_total_time=3)\n",
    "    try:\n",
    "        clf.fit(x, y)\n",
    "    except Exception as e:\n",
    "        print(\"Ignoring:\", e)\n",
    "        pass\n",
    "    \n",
    "    individual = Individual.from_string(\n",
    "        \"ExtraTreesClassifier(Normalizer(data, Normalizer.norm='l1'), ExtraTreesClassifier.bootstrap=True, ExtraTreesClassifier.criterion='gini', ExtraTreesClassifier.max_features=0.9500000000000001, min_samples_leaf=10, min_samples_split=13, ExtraTreesClassifier.n_estimators=100)\",\n",
    "        clf._pset,\n",
    "        clf._operator_set._safe_compile,  # notice the use of \"_safe_compile\" instead\n",
    "    )\n",
    "    pipeline = individual.pipeline\n",
    "    print(f\"{pipeline=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.datasets.dataset:Detected likely dependency issues when loading dataset 1430 from 'C:\\Users\\wiche\\.openml\\org\\openml\\www\\datasets\\1430\\dataset.pkl.py3'. This is most likely due to https://github.com/openml/openml-python/issues/918. Error message was: No module named 'scipy.sparse._csr'. We will continue loading data from the arff-file, but this will be much slower for big datasets. Please manually delete the cache file if you want OpenML-Python to attempt to reconstruct it.\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "\n",
    "dataset = openml.datasets.get_dataset(\"a9a\")\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    37155\n",
       " 1.0    11687\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
